{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6f87d942",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bd05ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Ollama server is running.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "MODEL_NAME = \"phi3:mini\"\n",
    "\n",
    "# check Ollama server is running\n",
    "try:\n",
    "    r = requests.get(\"http://localhost:11434/api/tags\", timeout=5)\n",
    "    r.raise_for_status()\n",
    "    print(\"✅ Ollama server is running.\")\n",
    "except Exception as e:\n",
    "    print(\"❌ Ollama server is not reachable.\")\n",
    "    print(\"Fix: run `ollama serve` in Terminal, then retry.\")\n",
    "    raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "fec0bb1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ollama_generate(\n",
    "    prompt: str,\n",
    "    model: str = MODEL_NAME,\n",
    "    max_tokens: int = 220,         # strict cap to avoid hanging\n",
    "    temperature: float = 0.2,\n",
    "    top_p: float = 0.9,\n",
    "    context_window: int = 2048,    # keep small for speed\n",
    "    timeout_sec: int = 60\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Uses Ollama local HTTP API (stable method).\n",
    "    Returns generated text response.\n",
    "    \"\"\"\n",
    "\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"num_predict\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"num_ctx\": context_window\n",
    "        }\n",
    "    }\n",
    "\n",
    "    r = requests.post(url, json=payload, timeout=timeout_sec)\n",
    "    r.raise_for_status()\n",
    "    data = r.json()\n",
    "\n",
    "    return data.get(\"response\", \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d7894510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Significant increase in global electric vehicle (EV) sales observed in 2023.\n",
      "\n",
      "- Sales surge primarily attributed to the markets of China and Europe.\n",
      "\n",
      "- The rise reflects growing consumer acceptance and advancements in EV technology.\n",
      "\n",
      "- Government policies promoting eco-friendly transportation also played a role.\n",
      "\n",
      "Sources: GlobalEVOutlook2024\n"
     ]
    }
   ],
   "source": [
    "test_prompt = \"\"\"\n",
    "You are VoltAI.\n",
    "\n",
    "RULES:\n",
    "- Answer ONLY using the evidence.\n",
    "- If evidence is insufficient, respond exactly: Insufficient data in knowledge base.\n",
    "- Use 4 bullet points max.\n",
    "- Keep response under 120 words.\n",
    "- End with: Sources: ...\n",
    "\n",
    "EVIDENCE:\n",
    "[SOURCE: GlobalEVOutlook2024, YEAR: 2024]\n",
    "Global EV sales rose significantly in 2023, mainly driven by China and Europe.\n",
    "\n",
    "QUESTION:\n",
    "Summarize EV sales growth in 2023.\n",
    "\"\"\".strip()\n",
    "\n",
    "response = ollama_generate(test_prompt)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "09eef13b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello!\n"
     ]
    }
   ],
   "source": [
    "def ollama_generate_fast(prompt: str) -> str:\n",
    "    return ollama_generate(\n",
    "        prompt=prompt,\n",
    "        max_tokens=160,\n",
    "        temperature=0.15,\n",
    "        top_p=0.9,\n",
    "        context_window=1536,\n",
    "        timeout_sec=45\n",
    "    )\n",
    "\n",
    "print(ollama_generate_fast(\"Say hello in one line.\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6086c86a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
