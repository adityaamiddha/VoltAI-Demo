{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aadae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install faiss-cpu sentence-transformers requests tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a08142f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityaamiddha/hello/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict\n",
    "\n",
    "INDEX_FILE = Path(\"vector_db/iea_faiss.index\")\n",
    "META_FILE  = Path(\"vector_db/iea_metadata.jsonl\")\n",
    "\n",
    "assert INDEX_FILE.exists(), \"FAISS index not found. Run vector_store.ipynb first.\"\n",
    "assert META_FILE.exists(), \"Metadata file not found. Run vector_store.ipynb first.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d079697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded FAISS index | vectors: 352\n",
      "‚úÖ Loaded metadata records: 352\n"
     ]
    }
   ],
   "source": [
    "index = faiss.read_index(str(INDEX_FILE))\n",
    "print(\"‚úÖ Loaded FAISS index | vectors:\", index.ntotal)\n",
    "\n",
    "metadata = []\n",
    "with META_FILE.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        metadata.append(json.loads(line))\n",
    "\n",
    "print(\"‚úÖ Loaded metadata records:\", len(metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e24d64ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded embedder: BAAI/bge-small-en-v1.5\n"
     ]
    }
   ],
   "source": [
    "EMBED_MODEL = \"BAAI/bge-small-en-v1.5\"\n",
    "embedder = SentenceTransformer(EMBED_MODEL)\n",
    "print(\"‚úÖ Loaded embedder:\", EMBED_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40c15c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"phi3:mini\"\n",
    "\n",
    "def ollama_generate(\n",
    "    prompt: str,\n",
    "    model: str = MODEL_NAME,\n",
    "    max_tokens: int = 220,\n",
    "    temperature: float = 0.2,\n",
    "    top_p: float = 0.9,\n",
    "    context_window: int = 2048,\n",
    "    timeout_sec: int = 60\n",
    ") -> str:\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"num_predict\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"num_ctx\": context_window\n",
    "        }\n",
    "    }\n",
    "    r = requests.post(url, json=payload, timeout=timeout_sec)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"response\", \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5df59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, k: int = 5) -> List[Dict]:\n",
    "    qv = embedder.encode(query, normalize_embeddings=True).astype(\"float32\")\n",
    "    D, I = index.search(np.array([qv]), k)\n",
    "\n",
    "    results = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "        item = metadata[idx].copy()\n",
    "        item[\"score\"] = float(score)\n",
    "        results.append(item)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "241e5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are VoltAI, a domain-specific assistant for electric vehicle market trends, charging infrastructure, battery ecosystem, and EV policy.\n",
    "You must answer ONLY using the provided EVIDENCE from the IEA Global EV Outlook knowledge base.\n",
    "\n",
    "Rules:\n",
    "1) Use ONLY information present in EVIDENCE.\n",
    "2) If the answer is not found in EVIDENCE, respond exactly with: \"Insufficient data in knowledge base.\"\n",
    "3) Be factual, concise, and avoid assumptions.\n",
    "4) When relevant, include numbers and year references.\n",
    "5) End with a Sources section listing sources used.\n",
    "\"\"\"\n",
    "\n",
    "def format_evidence(chunks: List[Dict], max_chars_per_chunk: int = 650) -> str:\n",
    "    formatted = []\n",
    "    for i, ch in enumerate(chunks, 1):\n",
    "        txt = ch[\"text\"].replace(\"\\n\", \" \").strip()\n",
    "        txt = txt[:max_chars_per_chunk]\n",
    "        formatted.append(\n",
    "            f\"[EVIDENCE {i}] (SOURCE={ch['source']}, YEAR={ch['year']}, CHUNK_ID={ch['chunk_id']}, SCORE={ch.get('score',0):.4f})\\n\"\n",
    "            f\"{txt}\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "def format_chat_history(chat_history: List[Dict], max_turns: int = 6) -> str:\n",
    "    if not chat_history:\n",
    "        return \"None\"\n",
    "    recent = chat_history[-max_turns:]\n",
    "    lines = []\n",
    "    for msg in recent:\n",
    "        role = msg[\"role\"].upper()\n",
    "        content = msg[\"content\"].strip()\n",
    "        lines.append(f\"{role}: {content}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def build_prompt(user_query: str, retrieved_chunks: List[Dict], chat_history: List[Dict] = None) -> str:\n",
    "    evidence_block = format_evidence(retrieved_chunks)\n",
    "    history_block = format_chat_history(chat_history or [])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "{SYSTEM_PROMPT}\n",
    "\n",
    "CHAT HISTORY:\n",
    "{history_block}\n",
    "\n",
    "EVIDENCE:\n",
    "{evidence_block}\n",
    "\n",
    "USER QUESTION:\n",
    "{user_query}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Use bullet points.\n",
    "- Keep answer under 140 words.\n",
    "- No hallucinations.\n",
    "- End with Sources: ...\n",
    "\"\"\"\n",
    "    return prompt.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05b137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voltai_answer(\n",
    "    user_query: str,\n",
    "    chat_history: List[Dict],\n",
    "    top_k: int = 5\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    {\n",
    "      \"answer\": str,\n",
    "      \"sources\": list,\n",
    "      \"evidence\": list of chunks (top-k),\n",
    "      \"chat_history\": updated chat history\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Retrieve evidence\n",
    "    retrieved = retrieve(user_query, k=top_k)\n",
    "\n",
    "    # 2) Build prompt using evidence + memory\n",
    "    prompt = build_prompt(user_query, retrieved, chat_history=chat_history)\n",
    "\n",
    "    # 3) Generate response with Ollama\n",
    "    answer = ollama_generate(prompt)\n",
    "\n",
    "    # 4) Collect sources\n",
    "    sources = []\n",
    "    for ch in retrieved:\n",
    "        src = {\"source\": ch[\"source\"], \"year\": ch[\"year\"], \"chunk_id\": ch[\"chunk_id\"], \"score\": ch[\"score\"]}\n",
    "        sources.append(src)\n",
    "\n",
    "    # 5) Update chat history\n",
    "    chat_history = chat_history + [\n",
    "        {\"role\": \"user\", \"content\": user_query},\n",
    "        {\"role\": \"assistant\", \"content\": answer}\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"sources\": sources,\n",
    "        \"evidence\": retrieved,\n",
    "        \"chat_history\": chat_history\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "967bc91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:\n",
      " - Increased government targets for EV adoption in major markets, including policy support for vehicle and battery manufacturing as well as critical mineral supply chains (EVIDENCE 1).\n",
      "\n",
      "- Global spending on electric cars by governments and consumers significantly increased to over USD 400 billion in 2022. Policy requirements are a key driver of electrification for companies (EVIDENCE 1).\n",
      "\n",
      "- Electric vehicle registrations remain concentrated, with just under 60% from China, about 25% from Europe, and around 10% from the United States in 2023. The rest are distributed among other regions including Rest of World (RoW) markets (EVIDENCE 3).\n",
      "\n",
      "Sources: EV Outlook IEA Global Electric Vehicle Market Trends - Evidence ID: GEVO2023_clean, Year: 2023; Global EV Outlook IEA ‚Äì Catching up\n",
      "\n",
      "SOURCES:\n",
      " [{'source': 'GEVO2023_clean', 'year': 2023, 'chunk_id': 'IEA_2023_000034', 'score': 0.8367905616760254}, {'source': 'GlobalEVOutlook2025_clean', 'year': 2025, 'chunk_id': 'IEA_2025_000044', 'score': 0.821131706237793}, {'source': 'GlobalEVOutlook2024_clean', 'year': 2024, 'chunk_id': 'IEA_2024_000008', 'score': 0.820374608039856}]\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "result = voltai_answer(\n",
    "    user_query=\"What are the key global EV adoption trends discussed in 2023?\",\n",
    "    chat_history=chat_history,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(\"ANSWER:\\n\", result[\"answer\"])\n",
    "print(\"\\nSOURCES:\\n\", result[\"sources\"][:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b9fe870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1 Answer:\n",
      " - In 2023, global electric vehicle (EV) sales are expected to reach nearly 14 million units, marking a significant increase of about 35% compared to the previous year and raising EV's market share to approximately 18%. This growth is supported by declining costs and strengthened policy support in key regions such as the United States.\n",
      "- The report highlights that China has set an ambitious target for NEV sales, aiming for a 50% share by 2025, which helped Xpeng Motors become one of its national EV leaders alongside BEVs and PHEVs (and includes fuel cell electric vehicles).\n",
      "- In the United States, first-quarter EV sales reached around 350,000 units in early 2024, which is nearly a 15% increase from the same period in the previous year. The growth of plug-in hybrid electric vehicle (PHEV) sa\n",
      "\n",
      "Turn 2 Answer:\n",
      " - Global EV Outlook predicts a significant increase in charging infrastructure by 2030, aiming for nearly 90 GW of public slow and almost 500 GW of fast installed capacity under the Stated Policies Scenario. This growth is essential to support rising electric vehicle (EV) adoption rates worldwide.\n",
      "- In China alone, charger costs have decreased by 67% from 2016 to 2019 due to economies of scale; however, grid upgrades may be necessary as the number of chargers surpasses current capacity limits. The report emphasizes strategic and integrated power grid planning for anticipatory infrastructure development that considers EV needs alongside other sectors' demands.\n",
      "- Sources: [EVIDENCE 1], [EVIDENCE 2], [EVIDENCE 3]\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "r1 = voltai_answer(\"What does the report say about EV sales growth in 2023?\", chat_history, top_k=5)\n",
    "chat_history = r1[\"chat_history\"]\n",
    "\n",
    "r2 = voltai_answer(\"What about charging infrastructure growth?\", chat_history, top_k=5)\n",
    "chat_history = r2[\"chat_history\"]\n",
    "\n",
    "print(\"Turn 1 Answer:\\n\", r1[\"answer\"][:800])\n",
    "print(\"\\nTurn 2 Answer:\\n\", r2[\"answer\"][:800])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71c35f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ VoltAI Demo Chat started\n",
      "Type your prompt as normal.\n",
      "Commands: /exit, /clear, /sources, /help\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "VoltAI:\n",
      " - Nearly 14 million electric cars are projected to be sold globally in 2023, marking a significant increase from the previous year and contributing to an estimated global sales share of around 18%. This growth is partly attributed to cost declines and strengthened policy support. (Source: IEA Global EV Outlook - GEVO2023_clean)\n",
      "- China's electric car market exceeded expectations in 2022, achieving a sales share of around 29% with the government meeting its target three years early by aiming for 20% new energy vehicle sales by 2025. (Source: IEA Global EV Outlook - GEVO2023_clean)\n",
      "- In China, Xpeng Motors has become a national electric vehicle frontrunner with NEVs comprising over half of all car registrations in the country by 2025. (Source: IEA Global EV Outlook - GEV \n",
      "\n",
      "--- Sources ---\n",
      "- GEVO2023_clean (2023) | IEA_2023_000010 | score=0.780\n",
      "- GEVO2023_clean (2023) | IEA_2023_000063 | score=0.725\n",
      "- GlobalElectricVehicleOutlook2022_clean (2022) | IEA_2022_000042 | score=0.722\n",
      "\n",
      "\n",
      "VoltAI:\n",
      " - In China, electric car market share increased from around 29% in 2022 to over half of all registrations by 2025 (Source: IEA Global EV Outlook - GEVO2023_clean and IEA Global EV Outlook - GEVO2025_clean).\n",
      "- In the United States, electric car sales grew from around 350,000 in early 2022 to nearly 1.2 million by mid-2024 (Source: IEA Global EV Outlook - GEVO2024_clean and IEA Global EV Outlook - GEVO2025_clean).\n",
      "- In Brazil, electric car sales increased from less than 30,000 in early 2022 to over 61,750 by mid-2024 (Source: IEA Global EV Outlook - GEVO2025_clean and Box 1 \n",
      "\n",
      "--- Sources ---\n",
      "- GlobalEVOutlook2024_clean (2024) | IEA_2024_000014 | score=0.712\n",
      "- GlobalEVOutlook2025_clean (2025) | IEA_2025_000012 | score=0.703\n",
      "- GEVO2023_clean (2023) | IEA_2023_000010 | score=0.702\n",
      "\n",
      "\n",
      "\n",
      "‚ö†Ô∏è Interrupted by user. Type /exit to quit.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------\n",
    "# VoltAI Jupyter Chat Demo Loop\n",
    "# ----------------------------\n",
    "\n",
    "chat_history = []   # conversation memory (context)\n",
    "\n",
    "print(\"\\n‚úÖ VoltAI Demo Chat started\")\n",
    "print(\"Type your prompt as normal.\")\n",
    "print(\"Commands: /exit, /clear, /sources, /help\\n\")\n",
    "\n",
    "show_sources = True     # toggle sources display\n",
    "top_k = 5               # how many evidence chunks to retrieve\n",
    "\n",
    "last_result = None      # stores last response for /sources command\n",
    "\n",
    "\n",
    "while True:\n",
    "    user_query = input(\"You: \").strip()\n",
    "\n",
    "    # --- Commands ---\n",
    "    if user_query.lower() in [\"/exit\", \"exit\", \"quit\", \"/quit\", \"/stop\"]:\n",
    "        print(\"\\n‚úÖ VoltAI Demo Chat ended.\")\n",
    "        break\n",
    "\n",
    "    if user_query.lower() in [\"/help\"]:\n",
    "        print(\"\"\"\n",
    "Commands:\n",
    "  /help     Show commands\n",
    "  /clear    Clear conversation memory\n",
    "  /sources  Show sources/evidence from last answer\n",
    "  /exit     Exit chat\n",
    "\"\"\")\n",
    "        continue\n",
    "\n",
    "    if user_query.lower() == \"/clear\":\n",
    "        chat_history = []\n",
    "        last_result = None\n",
    "        print(\"\\nüßπ Cleared chat memory.\\n\")\n",
    "        continue\n",
    "\n",
    "    if user_query.lower() == \"/sources\":\n",
    "        if last_result is None:\n",
    "            print(\"\\n‚ö†Ô∏è No previous answer yet.\\n\")\n",
    "        else:\n",
    "            print(\"\\n--- SOURCES (Top Retrieved Evidence) ---\")\n",
    "            for i, s in enumerate(last_result[\"sources\"][:top_k], 1):\n",
    "                print(f\"{i}. {s['source']} ({s['year']}) | {s['chunk_id']} | score={s['score']:.3f}\")\n",
    "            print(\"\\n--- EVIDENCE SNIPPETS ---\")\n",
    "            for i, ev in enumerate(last_result[\"evidence\"][:top_k], 1):\n",
    "                snippet = ev[\"text\"].replace(\"\\n\", \" \")[:350]\n",
    "                print(f\"[{i}] {ev['chunk_id']} | {ev['source']} ({ev['year']})\")\n",
    "                print(\"    \", snippet, \"...\\n\")\n",
    "        continue\n",
    "\n",
    "    # --- Normal Query ---\n",
    "    try:\n",
    "        result = voltai_answer(\n",
    "            user_query=user_query,\n",
    "            chat_history=chat_history,\n",
    "            top_k=top_k\n",
    "        )\n",
    "\n",
    "        # update memory + store last result\n",
    "        chat_history = result[\"chat_history\"]\n",
    "        last_result = result\n",
    "\n",
    "        # show answer\n",
    "        print(\"\\nVoltAI:\\n\", result[\"answer\"], \"\\n\")\n",
    "\n",
    "        # show sources after answer (optional)\n",
    "        if show_sources:\n",
    "            print(\"--- Sources ---\")\n",
    "            for s in result[\"sources\"][:3]:\n",
    "                print(f\"- {s['source']} ({s['year']}) | {s['chunk_id']} | score={s['score']:.3f}\")\n",
    "            print()\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n\\n‚ö†Ô∏è Interrupted by user. Type /exit to quit.\\n\")\n",
    "        continue\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"\\n‚ùå Error:\", str(e), \"\\n\")\n",
    "        continue\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
