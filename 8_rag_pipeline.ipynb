{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aadae8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip -q install faiss-cpu sentence-transformers requests tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a08142f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/adityaamiddha/hello/.conda/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "import requests\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from typing import List, Dict\n",
    "\n",
    "INDEX_FILE = Path(\"vector_db/iea_faiss.index\")\n",
    "META_FILE  = Path(\"vector_db/iea_metadata.jsonl\")\n",
    "\n",
    "assert INDEX_FILE.exists(), \"FAISS index not found. Run vector_store.ipynb first.\"\n",
    "assert META_FILE.exists(), \"Metadata file not found. Run vector_store.ipynb first.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d079697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded FAISS index | vectors: 352\n",
      "✅ Loaded metadata records: 352\n"
     ]
    }
   ],
   "source": [
    "index = faiss.read_index(str(INDEX_FILE))\n",
    "print(\"✅ Loaded FAISS index | vectors:\", index.ntotal)\n",
    "\n",
    "metadata = []\n",
    "with META_FILE.open(\"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        metadata.append(json.loads(line))\n",
    "\n",
    "print(\"✅ Loaded metadata records:\", len(metadata))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e24d64ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded embedder: BAAI/bge-small-en-v1.5\n"
     ]
    }
   ],
   "source": [
    "EMBED_MODEL = \"BAAI/bge-small-en-v1.5\"\n",
    "embedder = SentenceTransformer(EMBED_MODEL)\n",
    "print(\"✅ Loaded embedder:\", EMBED_MODEL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40c15c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"phi3:mini\"\n",
    "\n",
    "def ollama_generate(\n",
    "    prompt: str,\n",
    "    model: str = MODEL_NAME,\n",
    "    max_tokens: int = 220,\n",
    "    temperature: float = 0.2,\n",
    "    top_p: float = 0.9,\n",
    "    context_window: int = 2048,\n",
    "    timeout_sec: int = 60\n",
    ") -> str:\n",
    "    url = \"http://localhost:11434/api/generate\"\n",
    "    payload = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False,\n",
    "        \"options\": {\n",
    "            \"num_predict\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"num_ctx\": context_window\n",
    "        }\n",
    "    }\n",
    "    r = requests.post(url, json=payload, timeout=timeout_sec)\n",
    "    r.raise_for_status()\n",
    "    return r.json().get(\"response\", \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5df59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(query: str, k: int = 5) -> List[Dict]:\n",
    "    qv = embedder.encode(query, normalize_embeddings=True).astype(\"float32\")\n",
    "    D, I = index.search(np.array([qv]), k)\n",
    "\n",
    "    results = []\n",
    "    for score, idx in zip(D[0], I[0]):\n",
    "        if idx == -1:\n",
    "            continue\n",
    "        item = metadata[idx].copy()\n",
    "        item[\"score\"] = float(score)\n",
    "        results.append(item)\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "241e5d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are VoltAI, a domain-specific assistant for electric vehicle market trends, charging infrastructure, battery ecosystem, and EV policy.\n",
    "You must answer ONLY using the provided EVIDENCE from the IEA Global EV Outlook knowledge base.\n",
    "\n",
    "Rules:\n",
    "1) Use ONLY information present in EVIDENCE.\n",
    "2) If the answer is not found in EVIDENCE, respond exactly with: \"Insufficient data in knowledge base.\"\n",
    "3) Be factual, concise, and avoid assumptions.\n",
    "4) When relevant, include numbers and year references.\n",
    "5) End with a Sources section listing sources used.\n",
    "\"\"\"\n",
    "\n",
    "def format_evidence(chunks: List[Dict], max_chars_per_chunk: int = 650) -> str:\n",
    "    formatted = []\n",
    "    for i, ch in enumerate(chunks, 1):\n",
    "        txt = ch[\"text\"].replace(\"\\n\", \" \").strip()\n",
    "        txt = txt[:max_chars_per_chunk]\n",
    "        formatted.append(\n",
    "            f\"[EVIDENCE {i}] (SOURCE={ch['source']}, YEAR={ch['year']}, CHUNK_ID={ch['chunk_id']}, SCORE={ch.get('score',0):.4f})\\n\"\n",
    "            f\"{txt}\"\n",
    "        )\n",
    "    return \"\\n\\n\".join(formatted)\n",
    "\n",
    "def format_chat_history(chat_history: List[Dict], max_turns: int = 6) -> str:\n",
    "    if not chat_history:\n",
    "        return \"None\"\n",
    "    recent = chat_history[-max_turns:]\n",
    "    lines = []\n",
    "    for msg in recent:\n",
    "        role = msg[\"role\"].upper()\n",
    "        content = msg[\"content\"].strip()\n",
    "        lines.append(f\"{role}: {content}\")\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "def build_prompt(user_query: str, retrieved_chunks: List[Dict], chat_history: List[Dict] = None) -> str:\n",
    "    evidence_block = format_evidence(retrieved_chunks)\n",
    "    history_block = format_chat_history(chat_history or [])\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "{SYSTEM_PROMPT}\n",
    "\n",
    "CHAT HISTORY:\n",
    "{history_block}\n",
    "\n",
    "EVIDENCE:\n",
    "{evidence_block}\n",
    "\n",
    "USER QUESTION:\n",
    "{user_query}\n",
    "\n",
    "INSTRUCTIONS:\n",
    "- Use bullet points.\n",
    "- Keep answer under 140 words.\n",
    "- No hallucinations.\n",
    "- End with Sources: ...\n",
    "\"\"\"\n",
    "    return prompt.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e05b137",
   "metadata": {},
   "outputs": [],
   "source": [
    "def voltai_answer(\n",
    "    user_query: str,\n",
    "    chat_history: List[Dict],\n",
    "    top_k: int = 5\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "    {\n",
    "      \"answer\": str,\n",
    "      \"sources\": list,\n",
    "      \"evidence\": list of chunks (top-k),\n",
    "      \"chat_history\": updated chat history\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    # 1) Retrieve evidence\n",
    "    retrieved = retrieve(user_query, k=top_k)\n",
    "\n",
    "    # 2) Build prompt using evidence + memory\n",
    "    prompt = build_prompt(user_query, retrieved, chat_history=chat_history)\n",
    "\n",
    "    # 3) Generate response with Ollama\n",
    "    answer = ollama_generate(prompt)\n",
    "\n",
    "    # 4) Collect sources\n",
    "    sources = []\n",
    "    for ch in retrieved:\n",
    "        src = {\"source\": ch[\"source\"], \"year\": ch[\"year\"], \"chunk_id\": ch[\"chunk_id\"], \"score\": ch[\"score\"]}\n",
    "        sources.append(src)\n",
    "\n",
    "    # 5) Update chat history\n",
    "    chat_history = chat_history + [\n",
    "        {\"role\": \"user\", \"content\": user_query},\n",
    "        {\"role\": \"assistant\", \"content\": answer}\n",
    "    ]\n",
    "\n",
    "    return {\n",
    "        \"answer\": answer,\n",
    "        \"sources\": sources,\n",
    "        \"evidence\": retrieved,\n",
    "        \"chat_history\": chat_history\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "967bc91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:\n",
      " - Increased government targets for EV adoption in major markets, including policy support for vehicle and battery manufacturing as well as critical mineral supply chains (EVIDENCE 1).\n",
      "\n",
      "- Global spending on electric cars by governments and consumers significantly increased to USD 400 billion in 2022. Policy requirements were a key driver of electrification for companies, particularly within major markets such as China, Europe, and the United States (EVIDENCE 1).\n",
      "- Electric car registrations remain concentrated with just under 60% occurring in China, about 25% in Europe, and around 10% in the U.S., indicating regional disparities in EV adoption rates (EVIDENCE 3).\n",
      "\n",
      "Sources: IEA Global EV Outlook knowledge base - Evidence ID [IEA_2023_000034], [IEA_2025_000044], and\n",
      "\n",
      "SOURCES:\n",
      " [{'source': 'GEVO2023_clean', 'year': 2023, 'chunk_id': 'IEA_2023_000034', 'score': 0.8367905616760254}, {'source': 'GlobalEVOutlook2025_clean', 'year': 2025, 'chunk_id': 'IEA_2025_000044', 'score': 0.821131706237793}, {'source': 'GlobalEVOutlook2024_clean', 'year': 2024, 'chunk_id': 'IEA_2024_000008', 'score': 0.820374608039856}]\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "result = voltai_answer(\n",
    "    user_query=\"What are the key global EV adoption trends discussed in 2023?\",\n",
    "    chat_history=chat_history,\n",
    "    top_k=5\n",
    ")\n",
    "\n",
    "print(\"ANSWER:\\n\", result[\"answer\"])\n",
    "print(\"\\nSOURCES:\\n\", result[\"sources\"][:3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b9fe870",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turn 1 Answer:\n",
      " - In 2023, global electric vehicle (EV) sales are expected to reach nearly 14 million units, marking a significant increase of about 35% compared to the previous year and boosting EVs' market share to approximately 18%. This growth is supported by declining costs and strengthened policy support in key regions such as the United States.\n",
      "- The report highlights that China has set an ambitious target for NEV sales, aiming for a 50% share by 2025. Xpeng Motors' expansion into national EV leadership is attributed to this goal and includes BEVs, PHEVs, and fuel cell electric vehicles in their portfolio.\n",
      "- The United States saw first-quarter sales of around 350,000 units for the year, which was nearly a 15% increase from the same period in the previous year. Sales growth is particularly high amon\n",
      "\n",
      "Turn 2 Answer:\n",
      " - Global EV Outlook predicts a significant increase in charging infrastructure by 2030, aiming for nearly 90 GW of public slow and almost 500 GW of fast installed capacity under the Stated Policies Scenario. This growth is expected to accommodate over 8 million existing slow points and close to 5 million current fast chargers by then.\n",
      "- Charging infrastructure in China, which already has more than half a billion publicly accessible EV charge points (170 TWh of electricity), will continue expanding as the country seeks to meet its ambitious NEV sales targets and integrate with non-EV loads like electrified heating.\n",
      "- In Europe, strategic planning for grid upgrades is essential due to anticipated increases in EV charging demand alongside other energy services such as air conditioning and ele\n"
     ]
    }
   ],
   "source": [
    "chat_history = []\n",
    "\n",
    "r1 = voltai_answer(\"What does the report say about EV sales growth in 2023?\", chat_history, top_k=5)\n",
    "chat_history = r1[\"chat_history\"]\n",
    "\n",
    "r2 = voltai_answer(\"What about charging infrastructure growth?\", chat_history, top_k=5)\n",
    "chat_history = r2[\"chat_history\"]\n",
    "\n",
    "print(\"Turn 1 Answer:\\n\", r1[\"answer\"][:800])\n",
    "print(\"\\nTurn 2 Answer:\\n\", r2[\"answer\"][:800])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
